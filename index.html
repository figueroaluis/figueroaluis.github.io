<!DOCTYPE HTML>
<!--
	Phantom by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>
	<head>
		<title>Trajectory Forecasting | Gumparthi, Figueroa, & Vashisht</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="assets/css/main.css" />
		<noscript><link rel="stylesheet" href="assets/css/noscript.css" /></noscript>
	</head>
	<body class="is-preload">
		<!-- Wrapper -->
			<div id="wrapper">

				<!-- Main -->
					<div id="main">
						<div class="inner">
							<!-- Title, group members, course/year -->
							<br>
							<h1>Trajectory Forecasting Using Segmentation & Attention</h1>
							<p>Asish Gumparthi, Luis Figueroa, and Dhruv Vashisht | Course Project, Visual Learning and Recognition, Spring 2021</p>
							<p><a href = "https://github.com/figueroaluis/vlr-project-s2021">Code</a> | <a href = "https://trajectory-prediction.figueroaluis.com/">Website</a></p>

							<figure>
								<span class="image main">
									<img src="images/banner_image.png" alt="" />
									<figcaption><i>TODO: replace with our results</i> Figure 1. Qualitative human trajectory predictions from our framework.</figcaption>
								</span>
							</figure>

							<!-- Starting body content -->
							<h2>Motivation and Task</h2>
							<p>Trajectory Prediction is the problem of predicting the short-term and long-term spatial coordinates of various road-agents such as cars, buses, pedestrians etc. Accurate and robust trajectory prediction is critical 
							for the advancement of autonomous vehicles as it allows for better maneuverability in dense urban environments. As such, modeling pedestrian motion is non-trivial, as human motion paths are affected by environmental factors, 
							leading to many plausible ways by which people could move. These movements vary, especially in complicated scenes where pedestrians move around obstacles such as trees, garbage bins, or benches. In addition, occlusion presents an additional challenge when tracking pedestrians. 
							In this project, we wish to explore the use of segmentation information, in addition to a subject's movement and social interactions, to improve the trajectory prediction of pedestrians. Semantic segmentation can provide us with contextual information which can be used as a constraint to predict movements that are physically possible. 
							This can further be expanded to include only socially acceptable movements, e.g. movements on a sidewalk and not on top of a car.</p>
							<p>For this project, we define our task as follows:
							Given a scene with multiple human subjects, we observe the spatial coordinates of the person of interest for the first 5 steps and then predict their trajectory for the next 5 steps, or 2 seconds into the future
								</p>
							<!-- Framework/Pipeline content -->
							<h2>Framework</h2>
							<h4><i>Encoding Branches</i></h4>
							<figure>
								<span class="image main">
									<img src="images/framework_main_branches.png" alt="" />
									<figcaption>Figure 2. Three main branches of the baseline framework.</figcaption>
								</span>
							</figure>
							<p>We start by encoding the spatial coordinates of the subject. We first use a linear layer to encode each pair of coordinates into 64 features and pass it through a non linearity. These encoded features are then passed through a GRU at every training time step. The final output of the GRU acts as our person features.</p>
							<p>Next, we look at the social interactions of the person of interest. We encode these interactions by dividing the immediate vicinity of the subject into a circular or log-circular grid. The influence of other people around the subject is measured as a function of the distance and relative angle between them. Similar to the person encoding, this is then passed through a GRU at every training step and the final output of the GRU is taken as the group features for the subject.</p>
							<p>Finally, we encode the scene features of every sample. We do this by taking the last frame of the training sequence and passing it through the encoder of a pre-trained segmentation model. This gives up segmentation features for that particular scene. These features are then repeated over the training steps and passed through a GRU to get the final scene features.</p>

							<h4><i>Incorporating Attention</i></h4>
							<figure>
								<span class="image main">
									<center>
									<img style="width: 75%" src="images/attention.png" alt="" />
									<figcaption>Figure 3. The 2 attention mechanisms</figcaption>
									</center>
								</span>
							</figure>
							<p>We also experiment with 2 attention mechanisms, namely, Scene attention and Person attention</p>
							<p>Scene attention is a co-attention mechanism where the person and group features attend to the image features, thus generating person-scene attention features and group-scene attention features. These attention features are then concatenated and passed througha linear layer before being input to the decoder.</p>
							<p>Person attention, on the other hand, is a co-attention mechanism where the scene and group features attend to the person features, thus generating scene-person attention features and group-person attention features. Our intuition is to get the scene and group context for each time step of the subject's trajectory to aid the decoder's predictions. As before, the attention features are again conacatenated and passed through a linear layer before sent to the decoder.</p>

							<h4><i>Decoder</i></h4>
							<figure>
								<span class="image main">
									<center>
										<img style="width: 75%" src="images/decoder.png" alt="" />
										<figcaption>Figure 4. Decoder architecture</figcaption>
									</center>
								</span>
							</figure>
							<p>Finally, we bring it all together in the decoder. In the baseline, we simply sum the person, group and scene features to get a non-sequential decoder input. We then replicate this this input to resemble a 5 step sequence and pass it through a decoder GRU that gives us our prediction features at each step. These predictions features are passed through a linear layer to convert them into predicted coordinates and compared to the ground truth to calculate the mean squared error</p>
							<p>In the attention model, we apply the attention mechanisms explained above on the person, group and scene features. The attended features are then replicated to resemble a sequential input and treated to the same architecture explained above to get predicted coordinates.</p>



							<!-- Results -->
							<h2>Experiments and Results</h2>							
							<h4><i>Quantitative Results</i></h4>
							<p>For training, we use Mean squared error and we evaluate our models using Average displacement error (ADE) and Final displacement error (FDE), which are standard metrics in trajectory prediction tasks. ADE is the average of the euclidian distance between the predicted coordinates and the ground truth at every prediction step, whereas FDE is the Euclidian distance between the final predicted coordinate and final ground truth target for a given sample</p>
							<figure>
								<span class="image main">
									<center>
										<img style="width: 75%" src="images/ade_fde.png" alt="" />
										<figcaption>Figure 5. FDE & ADE</figcaption>
									</center>
								</span>
							</figure>
							<table style="width:100%">
								<tr>
								  <th>Model</th>
								  <th>FDE</th>
								  <th>ADE</th>
								</tr>
								<tr>
								  <td>Baseline</td>
								  <td>0.142</td>
								  <td>0.081</td>
								</tr>
								<tr>
									<td>ResNext Encoder + Person Attention</td>
									<td><b>0.124</b></td>
									<td><b>0.077</b></td>
								  </tr>
								  <tr>
									<td>ResNext Encoder + Scene Attention</td>
									<td>0.144</td>
									<td>0.086</td>
								  </tr>
							  </table>
							<h4><i>Qualitative Results</i></h4>

						</div>
					</div>

				<!-- Footer -->
					<footer id="footer">
						<div class="inner">
							<ul class="copyright">
								<li>&copy; Asish Gumparthi, Luis Figueroa, and Dhruv Vashisht 2021. All rights reserved</li><li>Design: <a href="http://html5up.net">HTML5 UP</a></li>
							</ul>
						</div>
					</footer>

			</div>

		<!-- Scripts -->
			<script src="assets/js/jquery.min.js"></script>
			<script src="assets/js/browser.min.js"></script>
			<script src="assets/js/breakpoints.min.js"></script>
			<script src="assets/js/util.js"></script>
			<script src="assets/js/main.js"></script>

	</body>
</html>
