<!DOCTYPE HTML>
<!--
	Phantom by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>
	<head>
		<title>Trajectory Forecasting | Gumparthi, Figueroa, & Vashisht</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="assets/css/main.css" />
		<noscript><link rel="stylesheet" href="assets/css/noscript.css" /></noscript>
	</head>
	<body class="is-preload">
		<!-- Wrapper -->
			<div id="wrapper">

				<!-- Main -->
					<div id="main">
						<div class="inner">
							<!-- Title, group members, course/year -->
							<br>
							<h1>Trajectory Forecasting Using Segmentation & Attention</h1>
							<p>Asish Gumparthi, Luis Figueroa, and Dhruv Vashisht | Course Project, Visual Learning and Recognition, Spring 2021</p>
							<center>
								<p><a href = "https://github.com/figueroaluis/TrajectoryPrediction">Code</a> | <a href = "https://trajectory-prediction.figueroaluis.com/">Website</a></p>
							</center>

							<figure>
								<span class="image main">
									<img src="images/banner_image.png" alt="" />
									<figcaption><i>TODO: replace with our results</i> Figure 1. Qualitative human trajectory predictions from our framework.</figcaption>
								</span>
							</figure>

							<!-- Starting body content -->
							<h2>Motivation and Task</h2>
							<p>Trajectory Prediction is the problem of predicting the short-term and long-term spatial coordinates of various
								road-agents such as cars, buses, pedestrians etc. Accurate and robust trajectory prediction is critical
								for the advancement of autonomous vehicles as it allows for better maneuverability in dense urban environments.
								As such, modeling pedestrian motion is non-trivial, as human motion paths are affected by environmental factors,
								leading to many plausible ways by which people could move. These movements vary, especially in complicated
								scenes where pedestrians move around obstacles such as trees, garbage bins, or benches. In addition,
								occlusion presents an additional challenge when tracking pedestrians.
								In this project, we wish to explore the use of segmentation information, in addition to a subject's movement and
								social interactions, to improve the trajectory prediction of pedestrians. Semantic segmentation can provide us with contextual
								information which can be used as a constraint to predict movements that are physically possible.
								This can further be expanded to include only socially acceptable movements, e.g. movements are allowed on a sidewalk and not on top of a car.</p>

							<p><i>For this project, the task of trajectory prediction is defined as followed:</i><br>
								Given a scene with multiple human subjects, we observe the spatial coordinates of the person of interest for the
								first 5 steps and then predict their trajectory for the next 5 steps, that is 2 seconds into the future.
							</p>

							<!-- Dataset -->
							<h2>Dataset</h2>

							<figure>
								<span class="image main">
									<center>
										<img style="width: 65%" src="images/dataset.png" alt="" />
										<figcaption>Figure 2. Example frames from the ETH dataset.</figcaption>
									</center>
								</span>
							</figure>

							<p>For this project, we use two of the most popular datasets on human trajectory prediction, namely the ETH and UCY datasets.
							These datasets contain videos with a fixed camera with a top-down view of a scene with pedestrians. ETH contains two subset
							video sequences (Hotel and Univ) while UCY has three sequences (Univ, Zara01, and Zara02). These sequences provide a myriad of
							pedestrian walking behaviors. In total, we collect 1050 samples, where each sample corresponds to a particular pedestrian
							whose features are of the shape 10x4, where 10 is the total number of frames associated with each pedestrian. Any pedestrian that
							appears in less then 10 associated frames is discarded. The last dimension comes comes from a set of (PedID, FrameID, X Y), where
							PedID is the Pedestrian ID, FrameID is the frame identification number, and X and Y corresponds to the normalized pixel coordinates
							of the pedestrian for a given frame.
							</p>

							<!-- Framework/Pipeline content -->
							<h2>Framework</h2>
							<h4><i>Encoding Branches</i></h4>
							<figure>
								<span class="image main">
									<img src="images/framework_main_branches.png" alt="" />
									<figcaption>Figure 2. Three main branches of the baseline framework.</figcaption>
								</span>
							</figure>
							<p>We start by encoding the spatial coordinates of the subject. We first use a linear layer to encode
								 each pair of coordinates into 64 features and pass it through a non linearity. These encoded features
								 are then passed through a GRU at every training time step. The final output of the GRU acts as our person features.</p>
							<p>Next, we look at the social interactions of the person of interest. We encode these interactions by dividing
								the immediate vicinity of the subject into a circular or log-circular grid. The influence of other people
								around the subject is measured as a function of the distance and relative angle between them. Similar to the
								person encoding, this is then passed through a GRU at every training step and the final output of the GRU
								is taken as the group features for the subject.
							</p>
							<p>Finally, we encode the scene features of every sample. We do this by taking the last frame of the
								training sequence and passing it through the encoder of a pre-trained segmentation model. This gives
								up segmentation features for that particular scene. These features are then repeated over the training
								steps and passed through a GRU to get the final scene features.
							</p>

							<h4><i>Incorporating Attention</i></h4>
							<figure>
								<span class="image main">
									<center>
									<img style="width: 75%" src="images/attention.png" alt="" />
									<figcaption>Figure 3. The two attention mechanisms.</figcaption>
									</center>
								</span>
							</figure>
							<p>We also experiment with two attention mechanisms, namely, <i>Scene attention</i> and <i>Person attention</i>.</p>
							<p>Scene attention is a co-attention mechanism where the person and group features attend to the image features,
								thus generating person-scene attention features and group-scene attention features. These attention features are
								then concatenated and passed througha linear layer before being input to the decoder.</p>
							<p>Person attention, on the other hand, is a co-attention mechanism where the scene and group features attend
								to the person features, thus generating scene-person attention features and group-person attention features. Our
								intuition is to get the scene and group context for each time step of the subject's trajectory to aid the decoder's
								predictions. As before, the attention features are again conacatenated and passed through a linear layer before sent to the decoder.</p>

							<h4><i>Decoder</i></h4>
							<figure>
								<span class="image main">
									<center>
										<img style="width: 75%" src="images/decoder.png" alt="" />
										<figcaption>Figure 4. Decoder architecture.</figcaption>
									</center>
								</span>
							</figure>

							<p>Finally, we bring it all together in the decoder. In the baseline, we simply sum the person, group,
								and scene features to get a non-sequential decoder input. We then replicate this input to resemble
								a 5 step sequence and pass it through a decoder GRU that gives us our prediction features at each step.
								These predictions features are passed through a linear layer to convert them into predicted coordinates
								and compared to the ground truth to calculate the Mean Squared Error.
							</p>
							<p>In the attention model, we apply the attention mechanisms explained above on the person, group, and scene
								features. The attended features are then replicated to resemble a sequential input and treated to the same
								architecture explained above to get predicted coordinates.
							</p>

							<!-- Results -->
							<h2>Experiments and Results</h2>
							<p>In this section, we discuss the evaluation metrics and report the results of our different experiments
								evaluated on the ETH/UCY datasets. We also report results from an ablation study used to validate our choice
								of the scene feature encoder.</p>

							<h4>Metrics</h4>
							<p>For training, we use Mean Squared Error and we evaluate our models using <b>Average Displacement Error (ADE)</b>
								and <b>Final Displacement Error (FDE)</b>, which are standard metrics in trajectory prediction tasks. ADE is
								the average of the Euclidian Distance between the predicted coordinates and the ground truth at every
								prediction step, whereas FDE is the Euclidian Distance between the final predicted coordinate and final
								ground truth target for a given sample.</p>
							<br>

							<figure>
								<span class="image main">
									<center>
										<img style="width: 65%" src="images/ade_fde.png" alt="" />
										<figcaption>Figure 5. Visualizated example of the trajectory prediction metrics Average
											Displacement Error (ADE) and Final Displacement Error (FDE).
										</figcaption>
									</center>
								</span>
							</figure>

							<h4>Experimental Setup</h4>
							<p>We conducted four experiments to explore the effects of changing the scene encoders and by excluding or
								adding attention mechanisms on person or group features. In our experiments, we define the <i>Baseline</i>
								model by that defined in <a href="https://ieeexplore.ieee.org/document/8354239"><i>Xue et al.</i></a>.
								The particular component to notice is that of the encoder for scene features. In this model, the Convolutional Neural
								Network (CNN) is defined by three convolutional layers. For our experiments, we swap this scene encoder for a more
								complex CNN architecture, namely <a href="https://arxiv.org/abs/1611.05431">ResNeXt</a>. We then experiment with adding different attention
								mechanisms on either the person or group features.
							</p>

							<h4><i>Main Quantitative Results</i></h4>
								<table style="width:100%">
									<caption><i>Experimental Results</i></caption>
									<tr>
									  <th>Model</th>
									  <th>FDE</th>
									  <th>ADE</th>
									</tr>
									<tr>
									  <td>Baseline</td>
									  <td>0.142</td>
									  <td>0.081</td>
									</tr>
									<tr>
										<td>ResNeXt Encoder + Person Attention</td>
										<td><b>0.124</b></td>
										<td><b>0.077</b></td>
									</tr>
									<tr>
										<td>ResNeXt Encoder + Scene Attention</td>
										<td>0.144</td>
										<td>0.086</td>
									</tr>
								</table>

							<p>In the table above, we compute the Average Displacement Error (ADE) and Final Displacement Error (FDE) and compare for different
								models using distinct scene information extraction and attention mechanisms. Our results show that swapping the feature encoder with
								ResNeXt and adding attention on the person features significantly reduced the ADE and FDE when compared to the baseline. Interestingly,
								adding attention on the group features achieved comparatively similar ADE and FDE when compared to the baseline, but not any lower.
							</p>

							<br>

							<h4><i>Main Qualitative Results</i></h4>

							<h4><i>CNN Ablation Study</i></h4>
							<p>For this project, we were interested in improving the extraction of segmentation information from a scene. As a result, when
							 studying the baseline which consisted of a CNN architecture of three convolutional layers (with maxpool and batch norm layers), we
							 proposed that swapping this encoder with more complex, pre-trained CNN architectures could provide better scene feature extraction.
							 In this ablation study, we compare three different popular CNN models: VGG-16, ResNet-18, and ResNeXt-50. Since we wish to compare
							 the scene feature extraction capabilities of these pre-trained models, we utilize a <a href="https://arxiv.org/abs/1511.00561">SegNet</a>
							 decoder to achieve the segmentation results. While the segmentation decoder could be any other semantic segmentation decoder
							 (e.g. <a href='https://arxiv.org/abs/1505.04597'>U-Net</a>, <a href="https://arxiv.org/pdf/1606.00915.pdf">DeepLab</a>), we utilize SegNet per the studies by
							 <a href="https://ieeexplore.ieee.org/document/8813801"><i>Syed et al</i></a>.
							</p>

							<table style="width:100%">
								<caption><i>Experimental Results</i></caption>
								<tr>
									<th>CNN Architecture</th>
									<th>Segmentation Decoder Head</th>
									<th>mIoU</th>
								</tr>
								<tr>
									<td>VGG-16</td>
									<td>SegNet</td>
									<td>87.7</td>
								</tr>
								<tr>
									<td>ResNet-18</td>
									<td>SegNet</td>
									<td>89.3</td>
								</tr>
								<tr>
									<td>ResNeXt-50</td>
									<td>SegNet</td>
									<td><b>91.3</b></td>
								</tr>
							</table>

							<p>In the table above, we show the results from the ablation study to choose a new encoder for the Trajectory Prediction model.
								For this study, we utilize three CNN architectures pre-trained on <a href="https://ieeexplore.ieee.org/document/5206848">ImageNet</a>.
								In <a href="https://ieeexplore.ieee.org/document/8813801"><i>Syed et al</i></a>, VGG-16 is selected as the feature extractor for the scene features.
								<i>Syed et al</i> argues that a CNN encoder trained for the task of semantic segmentation can provide better scene features for pedestrian trajectory
								predictions. Thus, we train and evaluate each model presented above on the <a>CamVid</a> dataset. Then, we evaluate the performance of each model
								by using the mean Intersection over Union (mIoU) score, which is the average IoU score of all classes. As shown, the segmentation model with
								a ResNeXt encoder achieves the greatest mIoU score. Therefore, we select ResNeXt our encoder of choice for the main experiments.
							</p>
							<center><iframe width="1120" height="630" src="https://www.youtube.com/embed/bt2_xwNjiYI" title="Trajectory Forecasting" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe></center>



						</div>
					</div>

				<!-- Footer -->
					<footer id="footer">
						<div class="inner">
							<ul class="copyright">
								<li>&copy; Asish Gumparthi, Luis Figueroa, and Dhruv Vashisht 2021. All rights reserved</li><li>Design: <a href="http://html5up.net">HTML5 UP</a></li>
							</ul>
						</div>
					</footer>

			</div>

		<!-- Scripts -->
			<script src="assets/js/jquery.min.js"></script>
			<script src="assets/js/browser.min.js"></script>
			<script src="assets/js/breakpoints.min.js"></script>
			<script src="assets/js/util.js"></script>
			<script src="assets/js/main.js"></script>

	</body>
</html>
